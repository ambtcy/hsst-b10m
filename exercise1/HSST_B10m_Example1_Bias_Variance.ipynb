{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5497ebb",
   "metadata": {},
   "source": [
    "# HSST B10m Exercise 1: Bias & Variance\n",
    "\n",
    "The following example is to demonstrate over and under-fitting a model, and to demonstrate the bias/variance trade-off.\n",
    "A cosine function is modelled using a polynomial fit, with the degree of the polynomial incremented from 1 to 15. The degree of the polynomial here is the model's hyperparameter. The Root Mean Squared Error (RMSE) is calculated for each model for the training data and an independent test data set. \n",
    "\n",
    "#### Pseudocode:\n",
    "<code>    X = generate N random points\n",
    "    y = apply cosine to X and add random noise\n",
    "    split X and y in half\n",
    "    for degree of polynomial from 1 to 15:\n",
    "        train polynomial model on training data\n",
    "        plot model vs expected for given degree of polynomial\n",
    "        calculate RMSE on training data\n",
    "        calculate RMSE on test data\n",
    "    plot training and test RMSE against degree of polynomial\n",
    "</code>\n",
    "\n",
    "#### Exercises:\n",
    "1. Experiment with changing the frequency of the cosine (parameter <code>B</code>) in the input model.\n",
    "2. Experiment with changing the size (<code>n_samples</code>) of the data set and test/train <code>split</code> on the error plots.\n",
    "3. Experiment with increasing the irreducible error (<code>noise</code>) variable.\n",
    "4. Extend the true_fun to a more complex relationship e.g. (<code>A * np.cos(B * np.pi * X + C) + A/2 * np.cos(2*B * np.pi * X + C)</code>\n",
    "\n",
    "#### Maths\n",
    "$RMSE=\\sqrt{\\dfrac{\\sum_{i=1}^{n}{(f(x_i) - y_i)^2}}{n}}$\n",
    "\n",
    "$y = A\\cos{(B\\pi x + C)} + \\epsilon$\n",
    "\n",
    "$f(x) = a_0 + a_1x + a_2x^2 + ... + a_{n-1}x^{n-1}+ a_nx^n$\n",
    "\n",
    "This example is adapted from <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\">Scikit-learn examples: Underfitting vs. Overfitting</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fcba2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the \"true\" function\n",
    "def true_fun(X, A=1, B=1.5, C=0):\n",
    "    return A * np.cos(B * np.pi * X + C)\n",
    "\n",
    "# random seed - ensures we have same model, change as desired\n",
    "np.random.seed(0)\n",
    "\n",
    "# define the size of the dataset - grow or shrink\n",
    "n_samples = 60\n",
    "\n",
    "# how to partition the test/train data set, 0.5 = 50% split.\n",
    "split = 0.5 \n",
    "\n",
    "# noise weight\n",
    "noise = 0.1\n",
    "\n",
    "# define the polynomial degrees to work through\n",
    "degrees = range(1,16,1)\n",
    "  \n",
    "    \n",
    "\n",
    "# create dataset and split\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * noise\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=split, random_state=None\n",
    ")\n",
    "\n",
    "# setup subplot grids\n",
    "subplot_width = 5\n",
    "subplot_height = 1+int(np.ceil(len(degrees)/subplot_width))\n",
    "plt.figure(figsize=(5*subplot_width, 5*subplot_height))\n",
    "\n",
    "# setup arrays to store the errors\n",
    "training_error = np.zeros(len(degrees))\n",
    "test_error = np.zeros(len(degrees))\n",
    "\n",
    "# iterate over polynomial degrees\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(subplot_height, subplot_width, i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    # create a model\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression),\n",
    "        ]\n",
    "    )\n",
    "    pipeline.fit(X_train[:, np.newaxis], y_train)\n",
    "\n",
    "    # create subplot of fit for each polynomial degree\n",
    "    X_show = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_show, pipeline.predict(X_show[:, np.newaxis]),color='firebrick',linewidth=3, label=\"Model\")\n",
    "    plt.plot(X_show, true_fun(X_show),color='darkgrey',linestyle='dashed', label=\"True function\")\n",
    "    plt.scatter(X_train, y_train, marker='o',color=\"black\", s=20, label=\"Train samples\")\n",
    "    plt.scatter(X_test, y_test, marker='o',color=\"black\",facecolors='none', s=20, label=\"Test samples\")\n",
    "    # formatting for plot\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\".format(degrees[i]))\n",
    "\n",
    "    # calculate the Root Mean Squared Error for a given point.\n",
    "    np.put(training_error,i,\n",
    "           mean_squared_error(y_train, pipeline.predict(X_train[:, np.newaxis]),squared=True)\n",
    "          )\n",
    "    np.put(test_error,i, \n",
    "           mean_squared_error(y_test, pipeline.predict(X_test[:, np.newaxis]),squared=True)\n",
    "          )\n",
    "    \n",
    "# show grid of fits\n",
    "plt.show()\n",
    "\n",
    "# Show plot of training vs test errors\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(degrees,training_error,color='steelblue',linewidth=3, label='Training Error')\n",
    "plt.plot(degrees,test_error,color='firebrick',linewidth=3,label='Test Error')\n",
    "plt.xlabel(\"Degree of polynomial\")\n",
    "plt.ylabel(\"RMS Error\")\n",
    "plt.yscale('log')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Training and Test Errors for polynomial fitting\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (2023)",
   "language": "python",
   "name": "python3-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
